---
title:  "[Paper Review] Chain-of-Thought Prompting Eliits Reasoning in Large Language Models"
excerpt: "Prompting의 중요성"

categories: [Paper Review, NLP]
tags:
  - [AI, 논문리뷰, 공부, Medical AI, NLP, Deep learning]

toc: true
toc_sticky: true

feature_text: |
  ## [Paper Review] 
  Chain-of-Thought Prompting Eliits Reasoning in Large Language Models
feature_image: "https://picsum.photos/2560/600?image=733"
image: "https://picsum.photos/2560/600?image=733"
---

## CoT란 ?

CoT는 "Chain-of-Thought Prompting Eliits Reasoning in Large Language Models"
라는 논문에서 나온 개념으로 Chain-of-Thought는 사고의 사슬이라고 함

사고의 사슬을 직관적으로 이해 해보자면 생각을 이어나가는 체인 정도로 해석될 수 있는데
최종적으로 목표하는 정답을 얻어내기 위해 모델에게 대화의 체인 정도를 알려주는 것으로 이해가 된다.

논문에서는 in-context few-shot learning이라고 언급이 되고 있고
이는 Fine-tuning과 같이 모델 파라미터의 조정 없이 추론 단계에서
더 나은 정답을 나타내주는 방법론이라고 할 수 있다.

본 논문에서는 어떻게 이렇게 간단항 방법이 자연스러운 추론을 이끌어내는지 예시를 통해 알아본다.

<img width="846" alt="스크린샷 2024-05-06 오후 4 58 00" src="https://github.com/Twlee95/Twlee95.github.io/assets/76574427/50bc3212-58eb-46ac-9255-aafdbc9dafb7">

위 그림과 같이 특정 정답 문장을 Prompting 해 줄 경우 모델이 최종 정답에 가까워지는 출력을 하는 것을 볼 수있다.

## 문제의식
LLM은 많은 분야에서 모델의 크기를 키움으로써 큰 성과를 달성했다.
하지만, 모델 크기만을 키우는 것이 산수, 상식, 기호적 추론에서 무조건 좋다는 것은 입증되지 않았다.

## 아이디어
  1) 산술적 추론에서는 Rationales를 생성함으로써 큰 성과를 보일 수 있음
* Rationales : 자연어가 문장을 추론할 때 근거로 사용하는 단어,
영화 평판 분류를 할 때 "이 영화는 재미있다"를 입력으로 사용할 경우 "재미있다"가 Rationale에 해당됨

  2) "in-context few-shot learning via prompting"
새로운 테스크에 대한 Fine-tuning없이 약간에 Input / Output Example에 대한 Prompt만으로
성능향사에 기여할 수 있다.


## 이전 방식의 제한 사항
  1) Rationale Dataset을 만들기는 매우 비용이 큼
  2) Traditional few-shot prompting method in Brown et al. (2020)
 - 추론 능력을 요하는 테스크에 대해 취약함
 - 언어모델 규모 증가가 직접적인 성능향상이 되지 않는 경우가 있음



## CoT Prompting이 가지는 매력적인 Properties
1. First, chain of thought, in principle, allows models to decompose multi-step problems into intermediate steps, which means that additional computation can be allocated to problems  that require more reasoning steps
2. Second, a chain of thought provides an interpretable window into the behavior of the model,  suggesting how it might have arrived at a particular answer and providing opportunities  to debug where the reasoning path went wrong (although fully characterizing a model’s  computations that support an answer remains an open question).  
3. Third, chain-of-thought reasoning can be used for tasks such as math word problems,  commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least  in principle) to any task that humans can solve via language.  
4. Finally, chain-of-thought reasoning can be readily elicited in sufficiently large off-the-shelf  language models simply by including examples of chain of thought sequences into the  exemplars of few-shot prompting.  

## Used Datasets
  - GSM8K, math word problems
  - SVAMP, Dataset math word  problems with varying structures
  - ASDiv, diverse math word  problems
  - AQuA , algebraic word problems
  - MAWPS,



## Used Models
 - GPT-3, correspond to InstructGPT models of 350M, 1.3B, 6.7B, and 175B parameters
 - LaMDA, which has models of 422M, 2B, 8B, 68B, and 137B parameters
 - PaLM, which has models of 8B, 62B, and 540B parameters.
 - UL2, 20B
 - CODEX

## 추론방법
 - We sample from the models via greedy decoding (though follow-up work shows chain-of-thought prompting can be improved by taking the majority final answer over many sampled generations)

#### Greedy Decoding이란?
- Decoding 단계에서 <start>로 시작하여  <end>로 끝날때 까지 가장 높은 확률을 가진 단어를 그 때 그 떄 추출하는 방법
- Beam Search 같은 방법보다 직관적인 이해가 쉬우며, 구현이 간단함

## exampler를 주는 방법
- LaMDA에는 5개의 Random seed를 통해 5번의 셔플된 Exampler가 입력으로 들어가게 되고 5개의 평균이 결과가 됨
- LaMDA 실험에서 5개 간의 큰 차이가 없었기 때문에 다른 모델에서는 Single exampler order로 실험



## 결론
1) First
- CoT는 작은 모델에 대해서는 효과가 없음을 실험결과로 보임
- 성능 차이를 보이기 위해서는 ~100B Parameters 이상이 되어야 함
- We qualitatively found that models of smaller scale produced fluent but illogical chains of thought, leading to lower performance than standard prompting.
2) Second
- CoT는 GSM8K (the dataset with the lowest baseline performance)와 같은 더 더 복잡한 문제에서 뛰어난 성능을 발휘함 (기존 성능의 두 배 이상)

3) Third,
- standard prompting에서 PaLM 540B이 SVAMP데이터셋에서 이미 State-of-the-arts를 달성했음에도 GSM8K,SVAMP,and MAWPS에서 CoT를 사용한 PaLM 540B가 최신의 성능을 달성했다 

4) To better understand why chain-of-thought prompting works, 
- we manually examined model generated chains of thought by LaMDA 137B for GSM8K. => 모델이 CoT를 생성한다는 것을 짐작해볼수 있음?? (지식이 부족하여 추측)
- 모델이 정답을 맞춘 50개 중에서 모델이 생성한 CoT는 단 2개만을 틀림 
- 모델이 잘못된 답을 낸 50개 중에서는 모델이 생성한 CoT중 46%가 정확했다. (barring minor mistakes (calculator error, symbol mapping error, or one reasoning step missing)

5) + Why does increasing model scale improve chain-of-thought prompting?
- its success cannot be predicted only by extrapolating the performance of small scale models, as chain of thought actually hurts performance for most models smaller than 10B parameters.
- 때문에 본 논문에서는 PaLM 62B 모델의 출력물에 대한 오류분석을 시행함 ==> 여기서 나온 오류들을 3가지로 분류했다
 * sementic understanding(20 errors)
 * one step missing (18 errors)
 * other errors (7 error) : The “other category” included hallucinations, repetitive outputs, and symbol mapping errors.
 위 분석은 LaMDA 모델에서 분석한 초기 분석법이다.

 <img width="944" alt="image" src="https://github.com/Twlee95/Twlee95.github.io/assets/76574427/db4a4125-23b4-4335-a84b-838a3ca0f80f">


 ## The reason why Small LLM fail in CoT
 - relatively easy symbol mapping tasks에서 좋지 못한 성능을 보임, 새로운 예시에 대한 일반화 성능을 요구하는 Symbol reasoning task에서도
 - small langage model이 내재적으로 산수에 약하다(충분한 산수 역할을 보이기 위해서는 큰 스케일의 모델을 요구한다.)
 - 결론적으로 다음 작업에서는 Pretraining Data, Model Architecture, Optimization Objective중 어느것이 원인이되어 추론성능을 향상시키는 지 볼것이다.

 ## Ablation Study
 - GSM8K Task는 CoT가 없이 General Prompting을 사용할 경우 사용하기에 충분하지 않은 성능을 보임